---
title: 'Homework #5'
author: "May Dixon"
date: "November 14, 2016"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

**Using the "KamilarAndCooperData.csv" dataset, run a linear regression looking at log(HomeRange_km2) in relation to log(Body_mass_female_mean) and report your β coeffiecients (slope and intercept).**

Summarizing data:
```{r}
d<-read.csv("https://raw.githubusercontent.com/difiore/ADA2016/master/KamilarAndCooperData.csv", header=TRUE, sep=",", stringsAsFactors = TRUE)
colnames(d)

m<-lm(data=d, log(HomeRange_km2)~ log(Body_mass_female_mean))
sm<- summary(m)
```
Beta coefficients: 
```{r}
sm$coefficients
```
So, 
βo=-9.44
βx=1.04

Home range size gets larger with female body size. 

**Then, use bootstrapping to sample from your data 1000 times with replacement, each time fitting the same model and calculating the same coefficients. This generates a sampling distribution for each β coefficient.**

The head of the output for the bootstrapped betas:
```{r}
beta.oh<- NULL
beta.x<-NULL
for (i in 1:1000) {
     
    mysamp<-d[sample(1:nrow(d), replace = TRUE),]
    l<- lm(data=mysamp, log(HomeRange_km2)~ log(Body_mass_female_mean))
    s<- summary(l)
    beta.oh[i] <- s$coefficients[1,1] # all the b0 values, extracted
    beta.x[i] <- s$coefficients[2,1]# all the bx values, extracted
    
}

betas<-cbind(beta.oh, beta.x) #dataframe of all beta0 and betaX's
head(betas)

```



**Estimate the standard error for each of your β coefficients as the standard deviation of the sampling distribution from your bootstrap and determine the 95% CI for each of your β coefficients based on the appropriate quantiles from your sampling distribution.**


Standard error of bootstrapped Bo and Bx:

```{r}
betas<- as.data.frame(betas)
betas$ss.b0 <- (betas$beta.oh-mean(betas$beta.oh))^2  #sum of squares b0
betas$ss.bx <- (betas$beta.x-mean(betas$beta.x))^2 #sum of squares bx
SD.B0<- sqrt(sum(betas$ss.b0)/(nrow(betas)-1)) # SD B0
SD.Bx<- sqrt(sum(betas$ss.bx)/(nrow(betas)-1)) #SD BX
tab<- cbind(SD.B0, SD.Bx)

colnames(tab)<- c("SE bootstrapped Bo", "SE bootstrapped Bx") #presenting nicer
tab

#SD.B0 
#SD.Bx
```


95% confidence intervals for Bo:
```{r}
library(reshape2)
#95%CI
#bo
lower.bo<- quantile(betas$beta.oh, probs=0.025)
upper.bo<- quantile(betas$beta.oh, probs=0.975)
lbo<-melt(lower.bo)
ubo<- melt(upper.bo)
bos<- rbind(lbo, ubo)
colnames(bos)<-("Bo value")
bos
#lower.bo
#upper.bo
```
So there is a 95% chance that the true value of the intercept falls between -10.7 and -8.4. (aa may vary sightly bc no set.seed()) 

95% confidence intervals for Bx:

```{r}
#bo

lower.bx<- quantile(betas$beta.x, probs=0.025)
upper.bx<- quantile(betas$beta.x, probs=0.975)
lbx<-melt(lower.bx)
ubx<- melt(upper.bx)
bxs<- rbind(lbx, ubx)
colnames(bxs)<-("Bx value")
bxs
#lower.bx
#upper.bx
```

We are 95% chance that the true slope is between 0.89 and 1.20 


**How does the former compare to the SE estimated from your entire dataset using the formula for standard error implemented in lm()?**
```{r}
# sm$coefficients[,"Std. Error"] #the SE from the original data
# SD.B0 #SE from bootstrapped data
# SD.Bx
bootSD<- cbind(SD.B0, SD.Bx)
SD.comp<- rbind(sm$coefficients[,"Std. Error"], bootSD)
rownames(SD.comp)<- c("SE of original data:", "SE of bootstrapped data:")
SD.comp
```
The standard errors are  very similar to the bootstrapped data, though a little smaller. So, just one sample set is perhaps pretty good at estimating the standard error

**How does the latter compare to the 95% CI estimated from your entire dataset?**
```{r}
# confint(m) #conf interval of original model
# lower.bo
# upper.bo
# lower.bx
# upper.bx

CI.bo <- cbind(lower.bo, upper.bo)
CI.bx <- cbind(lower.bx, upper.bx)
CI.boot <- rbind(CI.bo, CI.bx)
colnames(CI.boot)<- c("2.5%", "97.5%")
rownames(CI.boot)<-c("(Intercept bootstrapped", "log(Body_mass_female_mean) bootstrapped")
CI.all <- rbind(confint(m), CI.boot)
CI.all
```
The confidence intervals are fractionally smaller with the bootstrapped data, but practically the same. Again, the original model estimates the data pretty well without bootstrapping. 

EXTRA CREDIT: + 2

Write a FUNCTION that takes as its arguments a dataframe, "d", a linear model, "m" (as a character string, e.g., "logHR~logBM"), a user-defined confidence interval level, "conf.level" (with default = 0.95), and a number of bootstrap replicates, "n" (with default = 1000). Your function should return a dataframe that includes: beta coefficient names; beta coefficients, standard errors, and upper and lower CI limits for the linear model based on your entire dataset; and mean beta coefficient estimates, SEs, and CI limits for those coefficients based on your bootstrap.

v This was my stab at this but it didn't go far. Too much to do in life! 
```{r, eval=FALSE}

#nevermind, make it a loop later if I have time
mysample <- mydata[sample(1:nrow(mydata), 50,
  	replace=FALSE),]

n <-nrow(d)

bootz<-function(d, n=1000, m){
set <- list(NULL)  # sets up a dummy variable to hold our 10000 simulations
lmset <- NULL
for (i in 1:n) {
    set[i] <- d[sample(1:nrow(d), replace = TRUE),]
    lmset[i] <- lm(data=set[i], m)
    
}
return(head(lmset))
}
bootz(d, n=1000, m="log(HomeRange_km2)~ log(Body_mass_female_mean)")


```
EXTRA EXTRA CREDIT: + 1

Graph each beta value from the linear model and its corresponding mean value, lower CI and upper CI from a bootstrap as a function of number of bootstraps from 10 to 200 by 10s. HINT: the beta value from the linear model will be the same for all bootstraps and the mean beta value may not differ that much!